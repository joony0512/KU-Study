# CH8

## RNN(Recurrent Neural Network)의 원리

- RNN은 시퀀스 데이터를 처리하는 인공 신경망의 한 종류로, 이전 입력값의 정보를 기억하고 현재 입력값과 함께 처리하여 출력값을 생성합니다. 이러한 특성으로 RNN은 자연어 처리 및 컴퓨터 비전 분야에서 널리 사용됩니다.
- RNN은 입력층, 은닉층, 출력층으로 구성되어 있습니다. 은닉층은 현재 입력값과 이전 입력값의 정보를 기억하고, 출력층에서는 최종 출력값을 생성합니다. 이전 입력값의 정보를 기억하기 위해 RNN은 순환 구조를 사용합니다. 이러한 순환 구조는 은닉층의 출력값이 다시 은닉층의 입력값으로 들어가기 때문에, 이전 입력값의 정보를 현재 입력값과 함께 처리할 수 있게 됩니다.
- RNN은 입력값이 순서에 따라 처리되는 시퀀스 데이터를 다루기 때문에, 입력값과 출력값의 길이가 일정하지 않을 수 있습니다. 이러한 시퀀스 데이터를 처리하기 위해 RNN은 시계열 데이터를 처리하는데 특화된 모델로서, 다양한 응용 분야에서 사용됩니다.

## RNN의 구조

- RNN은 입력층, 은닉층, 출력층으로 구성되어 있습니다. 입력층은 시퀀스 데이터를 입력받는 역할을 하며, 은닉층은 이전 입력값의 정보를 기억합니다. 출력층은 최종 출력값을 생성합니다.
- 은닉층은 시퀀스 데이터의 입력값을 처리할 때, 이전 입력값의 정보를 함께 고려합니다. 이러한 특성으로 RNN은 순환 구조를 사용하며, 이전 입력값의 정보를 현재 입력값과 함께 처리할 수 있습니다. 이러한 구조로 RNN은 입력값과 출력값의 길이가 일정하지 않은 시퀀스 데이터를 처리하는데 특화된 모델로서, 다양한 응용 분야에서 사용됩니다.
- RNN은 순환 구조를 사용하기 때문에, 이전 입력값의 정보를 기억할 수 있습니다. 이러한 기억은 메모리 셀(Memory Cell)을 통해 이루어집니다. 메모리 셀은 현재 입력값과 이전 상태의 정보를 기반으로 필요한 정보만을 저장합니다. 이러한 구조로 RNN은 자연어 처리와 같은 다양한 응용 분야에서 높은 성능을 보입니다.
- RNN은 기울기 소실(Gradient Vanishing)과 폭주(Exploding) 문제를 가지고 있습니다. 이 문제는 RNN이 시퀀스 데이터를 처리할 때, 이전 입력값의 정보를 기억하기 위해 순환 구조를 사용하기 때문에 발생합니다.
- 기울기 소실은 역전파(backpropagation) 시 가중치가 제대로 업데이트 되지 않아 학습이 어려워지는 문제입니다. 이는 RNN의 역전파 시 체인룰(chain rule)을 사용하는데, 이전 시점의 기울기가 현재 시점으로 전달되어 곱해지면서 역전파 시 기울기가 지수적으로 감소하게 됩니다. 이렇게 되면 첫 번째 입력값을 학습하는데 있어서 기울기가 매우 작아져 학습이 잘 되지 않습니다.
- 반면, 폭주는 가중치가 급격하게 커져 학습이 불안정해지는 문제입니다. 이는 RNN이 입력값의 범위에 맞지 않는 가중치를 학습하여 가중치가 크게 증가하면서 발생합니다. 이렇게 되면 학습이 불안정해지고, 최적의 가중치를 찾기가 어렵게 됩니다.
- 이러한 기울기 소실과 폭주 문제를 해결하기 위해, 다양한 RNN의 변종이 개발되었습니다. LSTM과 GRU는 이러한 문제를 해결하고, 장기 의존성(Long-Term Dependency)을 학습할 수 있도록 설계된 모델입니다.

![image](https://github.com/joony0512/KU-Study/assets/109457820/de64c10a-d180-4d5a-b8f7-92caa1111600)
## LSTM

- LSTM(Long Short-Term Memory)은 RNN의 은닉층에 메모리 셀(Memory Cell)을 추가하여 불필요한 정보를 잊어버리고 필요한 정보만을 기억합니다. 메모리 셀은 입력 게이트, 삭제 게이트, 출력 게이트로 구성되어 있으며, 각 게이트는 현재 입력값과 이전 상태의 정보를 기반으로 필요한 정보만을 메모리 셀에 저장합니다.
- 이러한 구조로 인해 LSTM은 RNN의 기울기 소실과 폭주 문제를 해결할 수 있습니다.

## GRU

- GRU(Gated Recurrent Unit)는 LSTM과 달리 게이트가 2개로 구성되어 있으며, 업데이트 게이트와 리셋 게이트로 구성됩니다.
- 업데이트 게이트는 현재 입력값과 이전 상태의 정보를 기반으로 새로운 상태값을 계산하고, 리셋 게이트는 현재 입력값과 이전 상태의 정보를 기반으로 새로운 입력값을 계산합니다. 이러한 구조로 인해 GRU는 LSTM과 유사한 성능을 발휘합니다.
- GRU는 LSTM보다 간단한 구조를 가지고 있기 때문에 학습 속도가 빠르고, 적은 데이터로도 학습이 가능합니다.
- RNN, LSTM, GRU는 모두 시퀀스 데이터 처리에 특화된 모델이며, 자연어 처리 분야에서 많이 사용됩니다. 예를 들어, RNN을 이용하여 문장의 감성 분석을 수행하거나, LSTM을 이용하여 문장 생성 모델을 구현할 수 있습니다. 또한, GRU를 이용하여 자동 번역 시스템을 구현할 수도 있습니다.

![image](https://github.com/joony0512/KU-Study/assets/109457820/be072995-a3d9-4203-8171-4d9113e643b0)

## RNN의 응용 분야

- RNN은 자연어 처리 분야에서 주로 사용되며, 텍스트 생성, 기계 번역, 감성 분석, 자동 요약 등 다양한 작업에 적용됩니다. 텍스트 생성에서는 이전 단어를 기반으로 새로운 단어를 생성하며, 기계 번역에서는 입력 문장을 다른 언어로 번역합니다. 감성 분석에서는 입력된 텍스트가 긍정적인지 부정적인지 판단하며, 자동 요약에서는 긴 문장이나 글을 짧게 요약합니다.
- 또한 RNN은 음성 인식, 동영상 인식, 예측 분야에서도 사용됩니다. 음성 인식에서는 입력된 음성 데이터를 텍스트로 변환하며, 동영상 인식에서는 입력된 동영상 데이터를 이해하여 행동이나 사물 등을 인식합니다. 예측 분야에서는 시계열 데이터를 기반으로 다음 값을 예측합니다.

## RNN의 발전

- RNN은 기존의 피드포워드(feedforward) 신경망보다 시퀀스 데이터를 처리하는데 뛰어난 성능을 보여주었습니다. 하지만 기울기 소실과 폭주 문제로 인해 장기적인 시퀀스 데이터 처리에 한계가 있었습니다.
- 이를 해결하기 위해 LSTM과 GRU 같은 변형 모델들이 제안되었고, 더 나아가 Attention mechanism, Transformer, BERT 등의 모델들이 발전하면서 RNN의 한계를 극복하고 더욱 발전해 나가고 있습니다.

## 결론

- RNN은 시퀀스 데이터 처리에 탁월한 성능을 보이며, 다양한 분야에서 활용됩니다. 하지만 기울기 소실과 폭주 문제로 인해 장기적인 시퀀스 데이터 처리에 한계가 있습니다.
- 이를 해결하기 위해 LSTM, GRU 같은 변형 모델들이 등장하였고, 최근에는 Attention mechanism, Transformer, BERT 등의 모델들이 발전하면서 RNN의 한계를 극복하고 더욱 발전해 나가고 있습니다.
- 앞으로도 RNN을 비롯한 인공 신경망 기술의 발전은 다양한 분야에서 적용되어 더욱 발전할 것으로 기대됩니다.
