# Tree
- 트리(Tree)는 머신러닝 분야에서 가장 대표적인 알고리즘 중 하나입니다. 이는 입력 데이터를 분류하거나 예측하는 데에 매우 유용합니다. 트리는 그래프 형태로 표현되며, 각각의 노드(Node)는 특정 기준에 따라 데이터를 분할합니다. 이를 통해 결정 경계(Decision boundary)를 만들어냅니다.

- 트리는 주로 분류(Classification)나 회귀(Regression) 문제에서 사용됩니다. 분류 문제에서는 각 노드에서 데이터를 분류하여 결과값으로 해당 클래스를 반환합니다. 회귀 문제에서는 각 노드에서 데이터를 분할하여 결과값으로 해당 분할 구간의 평균값을 반환합니다.

## 트리의 장점

### 설명력: 
트리는 분류나 예측에 사용된 결정 경계를 시각화할 수 있기 때문에 결과를 해석하기 쉽습니다.
### 비선형 분류 문제 해결: 
결정 경계를 사용하여 비선형 분류 문제를 해결할 수 있습니다.
### 이상치(Outlier)에 둔감: 
트리는 이상치에 둔감하며, 이상치가 분류나 예측 결과에 미치는 영향이 상대적으로 적습니다.

## 트리의 단점

### 과적합(Overfitting) 문제: 
트리는 데이터의 불균형성이나 노이즈(Noise) 등에 민감하게 반응하여, 과적합 문제가 발생할 수 있습니다.
### 해석의 어려움: 
큰 규모의 트리는 시각화하기 어려우며, 이에 따라 해석이 어려울 수 있습니다.
### 데이터 변화에 민감함: 
데이터의 작은 변화에도 결정 경계가 크게 변화할 수 있습니다.
이러한 단점을 극복하기 위해 랜덤 포레스트(Random Forest)와 같은 알고리즘이 등장하게 되었습니다. 

# Random Forest

- 랜덤 포레스트(Random Forest)는 여러 개의 의사결정 트리(Decision Tree)를 만들어 그 결과를 집계하여 예측하는 알고리즘입니다. 
- 각 트리는 데이터의 일부분을 무작위로 선택하여 학습하고, 서로 다른 결정 경계를 생성합니다. 이러한 다양성은 모델의 성능과 안정성을 향상시키는 데 큰 역할을 합니다.

## Bagging
- 배깅(Bagging)은 랜덤 포레스트의 기반이 되는 알고리즘이며, 다양한 샘플링 기법을 사용하여 각각의 모델을 학습시키고 집계하여 결과를 만들어냅니다. 
- 이를 통해 모델의 분산을 줄이고 일반화 성능을 향상시킬 수 있습니다.

## Boosting
- 부스팅(Boosting)은 배깅과는 달리 이전 모델의 오차를 보완하는 방식으로 새로운 모델을 학습시키는 알고리즘입니다. 
- 예측값과 실제값 사이의 차이인 오차(residual)를 사용하여 새로운 모델을 학습시키고, 이를 반복적으로 수행하여 최종 예측값을 만들어냅니다. 
- 이러한 방식으로 분산을 줄이고 일반화 성능을 향상시키는 효과를 얻을 수 있습니다.

> 트리와 랜덤 포레스트는 데이터의 특성을 파악하여 예측하는 데에 특히 적합하며, 배깅과 부스팅은 모델의 안정성과 일반화 성능을 높이는 데 큰 역할을 합니다. 
> 또한 이러한 알고리즘들은 다양한 하이퍼파라미터를 조절하여 최적의 모델을 찾을 수 있는데, 이는 데이터의 특성에 따라 다르며, 적절한 하이퍼파라미터를 찾는 것이 모델의 성능을 향상시키는 데 중요합니다. 
> 또한 이러한 알고리즘들은 분류(Classification), 회귀(Regression), 이상치 탐지(Outlier detection) 등 다양한 문제에 적용될 수 있으며, 실제 산업에서도 활용되고 있습니다.

> 하지만 이러한 알고리즘들도 과적합(Overfitting) 문제가 발생할 수 있으며, 이를 방지하기 위해 교차 검증(Cross-validation) 등의 기법을 사용하여 모델의 일반화 성능을 평가해야 합니다. 
> 또한 데이터 전처리와 특성 선택 등의 과정도 중요하며, 이러한 과정을 적절히 수행하여 모델의 성능을 높일 수 있습니다.

> 따라서 머신러닝에서는 알고리즘 선택과 하이퍼파라미터 튜닝 뿐만 아니라, 데이터 전처리와 특성 선택 등의 과정도 매우 중요한 역할을 합니다. 이러한 전반적인 과정을 통해 최적의 모델을 만들어내는 것이 머신러닝의 핵심이며, 이를 통해 다양한 문제를 해결할 수 있습니다.
