Logistic Regression은 분류 문제를 해결하기 위한 머신러닝 알고리즘 중 하나입니다. 이 알고리즘은 데이터의 특징을 이용하여 해당 데이터가 어떤 클래스에 속하는지를 분류하는 모델을 만드는 데 사용됩니다. 

# 개념
Logistic Regression은 이름 그대로 로지스틱 함수를 이용하여 값을 0과 1 사이로 변환하고, 이 값을 확률로 해석하여 분류를 수행하는 알고리즘입니다. 일반적으로 이진 분류 문제를 다루지만, 다중 분류 문제에도 확장 가능합니다. 이 알고리즘은 선형 모델을 기반으로 하기 때문에, 선형 결합으로 데이터를 분류하는 것입니다.

# 학습 방법
Logistic Regression은 최적화 알고리즘인 경사 하강법(Gradient Descent)을 이용하여 모델을 학습합니다. 이때, 모델의 성능을 평가하기 위해 Log-Loss, 혹은 Cross-Entropy Loss라는 손실 함수를 이용합니다. 손실 함수는 모델이 예측한 값과 실제 값 사이의 차이를 계산하는 함수입니다.

# 확장 방법
Logistic Regression은 선형 결합으로 분류하는 것이기 때문에, 비선형 문제에는 적용할 수 없습니다. 이를 해결하기 위해 다항 로지스틱 회귀(Polynomial Logistic Regression)이나 로지스틱 회귀를 커널 기법으로 확장한 서포트 벡터 머신(Support Vector Machine) 등을 사용할 수 있습니다.

# 예제
이제 Logistic Regression을 이용하여 어떻게 분류 문제를 해결할 수 있는지 예제를 통해 알아보겠습니다. 예를 들어, 여러 사람의 키와 몸무게 데이터를 이용하여 성별을 분류하는 문제를 해결하고자 한다면, 키와 몸무게를 입력값으로 사용하여 Logistic Regression 모델을 학습시킬 수 있습니다. 학습된 모델은 새로운 데이터를 입력받았을 때, 해당 데이터의 특징을 이용하여 성별을 분류하는데 사용됩니다.

# 결론
Logistic Regression은 선형 모델을 기반으로 하여 구현이 간단하고, 효과적으로 분류 문제를 해결할 수 있는 머신러닝 알고리즘 중 하나입니다. 다만, 선형 결합으로 데이터를 분류하기 때문에 비선형 문제에 대해서는 한계가 있습니다. 이를 극복하기 위해 다항 로지스틱 회귀나 서포트 벡터 머신 등을 활용할 수 있습니다. Logistic Regression은 분류 문제뿐만 아니라, 이진 분류 문제에서 확률 값을 출력하는데 사용될 수 있기 때문에, 예측 결과의 신뢰도를 높일 수 있습니다. 따라서 Logistic Regression은 머신러닝 기초 개념을 이해하고 분류 문제를 다루는 데 중요한 알고리즘 중 하나입니다.

 

# <로지스틱 회귀에서 쓰이는 식들>


## 로지스틱 함수 (logistic function): Sigmoid 함수라고도 불리며, 입력 값을 0과 1 사이의 값으로 변환합니다.

sigmoid(z) = 1 / (1 + exp(-z))

## 선형 결합 (linear combination): 입력 데이터와 가중치 벡터의 내적을 계산하여 값을 출력합니다.

z = w^T * x

## 손실 함수 (loss function): 예측 결과와 실제 결과의 차이를 계산하여 모델의 오차를 평가합니다.

L(w) = - (1/m) * Σ y_i*log(h(x_i)) + (1-y_i)*log(1-h(x_i))

여기서, y_i는 실제 결과 값, h(x_i)는 모델이 예측한 결과 값입니다. 또한, m은 데이터 샘플의 수를 나타냅니다.

## 경사 하강법 (gradient descent): 손실 함수를 최소화하는 방향으로 가중치 값을 조정합니다.

w := w - α * ∇L(w)

여기서, α는 학습률을 나타내며, ∇L(w)는 손실 함수의 기울기를 나타냅니다.

 

# <로지스틱 회귀 예시>

## 공부 시간에 따른 시험 합격 여부를 예측하는 문제 

먼저, 입력 데이터로는 공부한 시간을 사용하고, 출력 데이터로는 시험 합격 여부(0 또는 1)를 사용합니다. 이를 바탕으로 로지스틱 회귀 모델을 학습시킵니다.

학습을 위해, 손실 함수와 경사 하강법을 사용합니다. 이를 통해 가중치(w) 값을 조정하며, 최적의 가중치 값을 찾습니다. 예측 결과와 실제 결과의 차이를 계산하여 모델의 오차를 평가하는 손실 함수는 다음과 같습니다.

L(w) = - (1/m) * Σ y_i*log(h(x_i)) + (1-y_i)*log(1-h(x_i))

여기서, y_i는 실제 결과 값(0 또는 1), h(x_i)는 모델이 예측한 결과 값입니다. 또한, m은 데이터 샘플의 수를 나타냅니다.

경사 하강법을 사용하여, 손실 함수를 최소화하는 방향으로 가중치 값을 조정합니다. 가중치의 초기값을 설정하고, 경사 하강법을 여러 번 반복하며 가중치를 업데이트합니다.

w := w - α * ∇L(w)

여기서, α는 학습률을 나타내며, ∇L(w)는 손실 함수의 기울기를 나타냅니다.

이렇게 학습된 모델을 이용하여 새로운 데이터를 예측할 수 있습니다. 예를 들어, 공부한 시간이 5시간일 때의 시험 합격 여부를 예측하고 싶다면, 입력 데이터로 5를 사용하여 모델을 실행하고, 출력 결과를 확인하면 됩니다.

이러한 방식으로 로지스틱 회귀 모델을 사용하여 다양한 문제를 해결할 수 있습니다.
